# Q-SSD: Quantized State Space Dual Architecture (é‡å­åŒ–çŠ¶æ€ç©ºé—´å¯¹å¶æ¶æ„)

Q-SSD æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨è§£å†³åæ‘©å°”å®šå¾‹æ—¶ä»£ AI è®¡ç®—çš„ç®—åŠ›ä¸å­˜å‚¨ç“¶é¢ˆã€‚å®ƒç»§æ‰¿äº†è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰çš„çº¿æ€§æ—¶é—´å¤æ‚åº¦å’ŒçŠ¶æ€è®°å¿†ç‰¹æ€§ï¼ŒåŒæ—¶é‡‡ç”¨å¯¹ç°ä»£ GPU/TPU å‹å¥½çš„å¯†é›†å¼ é‡è¿ç®—ä¸ 1.58-bit æå€¼é‡åŒ–ã€‚é¡¹ç›®ç›®æ ‡æ˜¯æä¾›ä¸€ç§â€œè¶…è¶Šç¥ç»å½¢æ€è®¡ç®—çš„ç¡…åŸºèåˆèŒƒå¼â€ï¼Œé€šè¿‡æŠ½å– SNN çš„è®¡ç®—æœ¬è´¨ï¼Œå®ç° $O(N)$ æ¨ç†å¤æ‚åº¦ã€$O(1)$ æ˜¾å­˜å ç”¨ï¼Œä»¥åŠç±»çªè§¦çš„æ•´æ•°åŠ æ³•è®¡ç®—ã€‚

## ç›®å½• (Table of Contents)
- [èƒŒæ™¯ä¸åŠ¨æœº (Background)](#èƒŒæ™¯ä¸åŠ¨æœº-background)
- [æ ¸å¿ƒç‰¹æ€§ (Key Features)](#æ ¸å¿ƒç‰¹æ€§-key-features)
- [æ¶æ„æ¦‚è§ˆ (Architecture)](#æ¶æ„æ¦‚è§ˆ-architecture)
- [æ€§èƒ½å¯¹æ¯” (Performance)](#æ€§èƒ½å¯¹æ¯”-performance)
- [å®‰è£…æŒ‡å— (Installation)](#å®‰è£…æŒ‡å—-installation)
- [ä½¿ç”¨è¯´æ˜ (Usage)](#ä½¿ç”¨è¯´æ˜-usage)
- [Roadmap](#roadmap)
- [å¼•ç”¨ (References)](#å¼•ç”¨-references)
- [è®¸å¯è¯ (License)](#è®¸å¯è¯-license)

## èƒŒæ™¯ä¸åŠ¨æœº (Background)
- Transformer ç“¶é¢ˆï¼šæ¨ç†å¤æ‚åº¦ä¸º $O(N^2)$ï¼ŒKV Cache ä½¿æ˜¾å­˜éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚
- SNN å›°å¢ƒï¼šç¨€ç–ã€å¼‚æ­¥ã€æ¡ä»¶åˆ†æ”¯ï¼ˆif/elseï¼‰å¯¼è‡´ Warp Divergence ä¸éåˆå¹¶å†…å­˜è®¿é—®ï¼Œéš¾ä»¥åœ¨ GPU ä¸Šé«˜æ•ˆè¿è¡Œã€‚
- Q-SSD æ–¹æ¡ˆï¼šæ‘’å¼ƒ LIF ç¥ç»å…ƒï¼Œé‡‡ç”¨é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSelective SSMsï¼‰å¹¶å¼•å…¥ BitNet b1.58ï¼Œå°† SNN çš„â€œçŠ¶æ€å‹ç¼©â€ä¼˜åŠ¿ä¸ GPU Tensor Core çš„å¯†é›†è¿ç®—æ•ˆç‡ç»“åˆã€‚

## æ ¸å¿ƒç‰¹æ€§ (Key Features)
- âš¡ï¸ çº¿æ€§æ¨ç†å¤æ‚åº¦ï¼šæ¨ç†æ—¶é—´ $O(N)$ï¼Œå¹¶è¡Œæ‰«æç®—æ³•ä½¿è®­ç»ƒå…·å¤‡ $O(\log N)$ å¹¶è¡Œæ•ˆç‡ã€‚
- ğŸ’¾ æ’å®šæ˜¾å­˜å ç”¨ï¼šå½»åº•æ¶ˆé™¤ KV Cacheï¼Œæ˜¾å­˜å ç”¨ä¸åºåˆ—é•¿åº¦æ— å…³ï¼Œä»…å–å†³äºæ¨¡å‹å‚æ•°ã€‚
- ğŸ”‹ 1.58-bit æƒé‡ (BitNet)ï¼šæƒé‡çº¦æŸä¸º $\{-1, 0, +1\}$ï¼ŒçŸ©é˜µä¹˜æ³•é€€åŒ–ä¸ºæµ®ç‚¹åŠ å‡æ³•ï¼Œæ˜¾è‘—é™ä½ç®—æœ¯èƒ½è€—ã€‚
- ğŸ‘ï¸ Event2Vec åµŒå…¥ï¼šé’ˆå¯¹ DVS äº‹ä»¶æµçš„å‘é‡åŒ–åµŒå…¥å±‚ï¼Œç¼“è§£ç¨€ç–äº‹ä»¶ä¸å¯†é›†è®¡ç®—å•å…ƒçš„é˜»æŠ—åŒ¹é…é—®é¢˜ã€‚
- ğŸ—ï¸ ç¡¬ä»¶åŸç”Ÿï¼šä¸º SIMT æ¶æ„è®¾è®¡ï¼Œæ—  Warp Divergenceï¼Œæ— éšæœºå†…å­˜è®¿é—®ã€‚

## æ¶æ„æ¦‚è§ˆ (Architecture)
Q-SSD ç”±å †å çš„ Quantized State Space Block æ„æˆï¼Œæ¯ä¸ª Block åŒ…å«å¦‚ä¸‹ç»„ä»¶ï¼š

### Quantized State Space Mixer
- æ›¿ä»£ Transformer Self-Attentionï¼Œè´Ÿè´£æ—¶é—´ç»´åº¦æ··åˆã€‚
- BitLinear æŠ•å½±ï¼šå°†è¾“å…¥ $x$ æŠ•å½±ä¸º $z, B, C$ ç­‰åˆ†é‡ï¼Œä»…æ¶‰åŠæ•´æ•°åŠ å‡ã€‚
- Short Convï¼šä¸€ç»´çŸ­å·ç§¯ï¼Œä¿ç•™ç¦»æ•£åŒ–è¿‡ç¨‹ä¸­çš„é«˜é¢‘å±€éƒ¨ä¿¡æ¯ã€‚
- SSM Coreï¼ˆFP16/BF16ï¼‰ï¼šé€’å½’è®¡ç®— $h_t = \bar{A}_t h_{t-1} + \bar{B}_t x_t$ï¼Œä¸ºæ•°å€¼ç¨³å®šæ€§ä¿ç•™é«˜ç²¾åº¦ã€‚
- Gate & Outputï¼šSiLU é—¨æ§ä¸ BitLinear è¾“å‡ºæŠ•å½±ã€‚

### Quantized Channel Mixer
- æ›¿ä»£ Transformer FFNã€‚
- ç»“æ„ï¼šGLU å˜ä½“ï¼ˆSwiGLU/GeGLUï¼‰å®ç°é€šé“æ··åˆã€‚
- æ‰©å±•ä¸å‹ç¼©ï¼šä½¿ç”¨ 1.58-bit BitLinear å°†ç»´åº¦æ‰©å±•è‡³ $4d$ å†å‹ç¼©å› $d$ã€‚

### é‡å­åŒ–ç­–ç•¥ (Quantization Strategy)
- æƒé‡ï¼šä½¿ç”¨ Absmean é‡åŒ–å°†æƒé‡æ˜ å°„è‡³ $\{-1, 0, 1\}$ã€‚
- æ¿€æ´»ï¼šé‡‡ç”¨å¹³æ»‘æ¢¯åº¦è¡¥å¿ã€æ—‹è½¬å˜æ¢ï¼ˆRotationï¼‰ä¸åˆ†å¸ƒå¯¹é½ï¼Œç¼“è§£ Mamba æ¶æ„ä¸­çš„æ¿€æ´»å€¼ç¦»ç¾¤ç‚¹ã€‚

### Event2Vec è¾“å…¥å±‚
- é¢å‘å¼‚æ­¥äº‹ä»¶æµ $(x, y, t, p)$ã€‚
- é€šè¿‡å‚æ•°åŒ–ç©ºé—´åµŒå…¥ä¸æ—¶é—´å·ç§¯åµŒå…¥ï¼Œå°†äº‹ä»¶æµæ˜ å°„ä¸ºå¯†é›†å‘é‡åºåˆ— $E$ï¼Œå®ç°â€œç¨€ç–è¾“å…¥ â†’ å¯†é›†è®¡ç®—â€ã€‚

## æ€§èƒ½å¯¹æ¯” (Performance)
| ç‰¹æ€§ | Transformer (LLM) | SNN (LIF) | Q-SSD (Proposed) |
| :--- | :---------------- | :-------- | :--------------- |
| æ—¶é—´å¤æ‚åº¦ | $O(N^2)$ | $O(1)$ | $O(1)$ |
| æ˜¾å­˜å¢é•¿ | çº¿æ€§ï¼ˆKV Cacheï¼‰ | æ’å®š | æ’å®š |
| è®¡ç®—èŒƒå¼ | FP16 ä¹˜ç´¯åŠ  | ç¨€ç–ç´¯åŠ ï¼ˆGPU æ•ˆç‡ä½ï¼‰ | Int8/1.58-bit åŠ æ³•ï¼ˆGPU æ•ˆç‡é«˜ï¼‰ |
| èƒ½æ•ˆï¼ˆç›¸å¯¹ï¼‰ | ~1.1 pJï¼ˆFP16 Multï¼‰ | ~0.x pJï¼ˆç†è®ºå€¼ï¼‰ | ~0.03 pJï¼ˆInt Addï¼‰ |

## å®‰è£…æŒ‡å— (Installation)
ç¯å¢ƒè¦æ±‚ï¼š
- Python 3.10+
- PyTorch 2.0+ï¼ˆæ¨è CUDA æ”¯æŒï¼‰
- Tritonï¼ˆç”¨äºä¼˜åŒ– Kernelï¼‰
- mamba-ssm / causal-conv1d

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/Q-SSD.git
cd Q-SSD

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n qssd python=3.10
conda activate qssd

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install "causal-conv1d>=1.2.0"
pip install "mamba-ssm>=1.2.0"
pip install -r requirements.txt
```

## ä½¿ç”¨è¯´æ˜ (Usage)

### 1. æ¨¡å‹å®šä¹‰ (Model Definition)
```python
from qssd.models import QSSDModel
from qssd.config import QSSDConfig

# åˆå§‹åŒ–é…ç½®ï¼ˆç±»ä¼¼ Mamba + BitNetï¼‰
config = QSSDConfig(
    d_model=512,
    n_layer=12,
    vocab_size=10000,
    ssm_cfg={"d_state": 16, "d_conv": 4, "expand": 2},
    quantization_mode="1.58bit",
)

model = QSSDModel(config).cuda()

# å‰å‘ä¼ æ’­
x = torch.randint(0, 10000, (1, 1024)).cuda()
logits = model(x)
print(logits.shape)  # torch.Size([1, 1024, 10000])
```

### 2. å¤„ç†ç¥ç»å½¢æ€æ•°æ® (Event Processing)
```python
from qssd.layers import Event2Vec

# æ¨¡æ‹Ÿ DVS äº‹ä»¶æµ (Batch, Time, H, W, Polarity)
events = torch.randn(1, 100, 64, 64, 2).cuda()

# å‘é‡åŒ–åµŒå…¥
e2v = Event2Vec(resolution=(64, 64), dim=512)
embeddings = e2v(events)

# ä¼ å…¥ä¸»ç½‘ç»œ
output = model(embeddings)
```

## Roadmap
- [ ] Phase 1: æ ¸å¿ƒæ¨¡å—å®ç°ï¼ˆBitLinear, Q-SSM Blockï¼‰
- [ ] Phase 2: Event2Vec åµŒå…¥å±‚å®ç°ä¸ DVS æ•°æ®é›†é€‚é…
- [ ] Phase 3: åœ¨ CUDA ä¸Šå®ç°ä¼˜åŒ–çš„ 1.58-bit Kernelï¼ˆTritonï¼‰
- [ ] Phase 4: åœ¨ ImageNet/CIFAR å’Œ NLP æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒéªŒè¯

## å¼•ç”¨ (References)
- Mamba: Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. ArXiv.
- BitNet b1.58: Ma, S., et al. (2024). The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits. ArXiv.
- Bi-Mamba: Towards Accurate 1-Bit State Space Models. ArXiv.
- Event2Vec: Processing Neuromorphic Events directly by Representations in Vector Space. ArXiv.

## è®¸å¯è¯ (License)
This project is licensed under the MIT License.
